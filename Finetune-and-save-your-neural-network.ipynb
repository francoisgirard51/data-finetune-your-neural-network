{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 15:34:19.192371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# KERAS\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune your Neural Network and Save it\n",
    "\n",
    "üéØ **Goals of this challenge**\n",
    "\n",
    "1. ‚öôÔ∏è **Fine-tune the optimizer** of a Neural Network\n",
    "2. üíæ **Save**/**Load** a trained Neural Network\n",
    "\n",
    "üë©üèª‚Äçüè´ Now that you have solid foundations about what Neural Networks are, how to design their architecture, and how to prevent them from overfitting, let's take a closer look at the `.compile(loss = ?, metrics = ?, activation = ?)` part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) The Boston Housing Dataset\n",
    "\n",
    "üìö `Tensorflow.Keras` has several built-in datasets that you can find [here](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)\n",
    "\n",
    "üè† Out of those, we are going to use the **Boston Housing Dataset**.\n",
    "\n",
    "Our mission is to **predict the values of the houses in USD (thousands)**, and we will measure the performance of our models using the _Mean Absolute Error (MAE)_ metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.1) Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57026/57026 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset:\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((404, 13), (404,)), ((102, 13), (102,)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the dataset: \n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.2) Quick Glance at the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of the houses' prices in the training set\n",
    "sns.histplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404 entries, 0 to 403\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       404 non-null    float64\n",
      " 1   1       404 non-null    float64\n",
      " 2   2       404 non-null    float64\n",
      " 3   3       404 non-null    float64\n",
      " 4   4       404 non-null    float64\n",
      " 5   5       404 non-null    float64\n",
      " 6   6       404 non-null    float64\n",
      " 7   7       404 non-null    float64\n",
      " 8   8       404 non-null    float64\n",
      " 9   9       404 non-null    float64\n",
      " 10  10      404 non-null    float64\n",
      " 11  11      404 non-null    float64\n",
      " 12  12      404 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 41.2 KB\n"
     ]
    }
   ],
   "source": [
    "# Null values and types of each feature:\n",
    "pd.DataFrame(X_train).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.745111</td>\n",
       "      <td>11.480198</td>\n",
       "      <td>11.104431</td>\n",
       "      <td>0.061881</td>\n",
       "      <td>0.557356</td>\n",
       "      <td>6.267082</td>\n",
       "      <td>69.010644</td>\n",
       "      <td>3.740271</td>\n",
       "      <td>9.440594</td>\n",
       "      <td>405.898515</td>\n",
       "      <td>18.475990</td>\n",
       "      <td>354.783168</td>\n",
       "      <td>12.740817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.240734</td>\n",
       "      <td>23.767711</td>\n",
       "      <td>6.811308</td>\n",
       "      <td>0.241238</td>\n",
       "      <td>0.117293</td>\n",
       "      <td>0.709788</td>\n",
       "      <td>27.940665</td>\n",
       "      <td>2.030215</td>\n",
       "      <td>8.698360</td>\n",
       "      <td>166.374543</td>\n",
       "      <td>2.200382</td>\n",
       "      <td>94.111148</td>\n",
       "      <td>7.254545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.081437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>5.874750</td>\n",
       "      <td>45.475000</td>\n",
       "      <td>2.077100</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.225000</td>\n",
       "      <td>374.672500</td>\n",
       "      <td>6.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.268880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.198500</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>3.142300</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>11.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.674808</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>6.609000</td>\n",
       "      <td>94.100000</td>\n",
       "      <td>5.118000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.157500</td>\n",
       "      <td>17.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.725000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>10.710300</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean     3.745111   11.480198   11.104431    0.061881    0.557356    6.267082   \n",
       "std      9.240734   23.767711    6.811308    0.241238    0.117293    0.709788   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.081437    0.000000    5.130000    0.000000    0.453000    5.874750   \n",
       "50%      0.268880    0.000000    9.690000    0.000000    0.538000    6.198500   \n",
       "75%      3.674808   12.500000   18.100000    0.000000    0.631000    6.609000   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.725000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean    69.010644    3.740271    9.440594  405.898515   18.475990  354.783168   \n",
       "std     27.940665    2.030215    8.698360  166.374543    2.200382   94.111148   \n",
       "min      2.900000    1.129600    1.000000  188.000000   12.600000    0.320000   \n",
       "25%     45.475000    2.077100    4.000000  279.000000   17.225000  374.672500   \n",
       "50%     78.500000    3.142300    5.000000  330.000000   19.100000  391.250000   \n",
       "75%     94.100000    5.118000   24.000000  666.000000   20.200000  396.157500   \n",
       "max    100.000000   10.710300   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "               12  \n",
       "count  404.000000  \n",
       "mean    12.740817  \n",
       "std      7.254545  \n",
       "min      1.730000  \n",
       "25%      6.890000  \n",
       "50%     11.395000  \n",
       "75%     17.092500  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistics about the numerical columns\n",
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.3) Minimal Data Preprocessing\n",
    "\n",
    "üëâ Here, we don't have any duplicates or missing values. Let's do the bare minimum of data preprocessing, i.e. ***scaling**, and move on quickly to the modeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùìScaling your Features\n",
    "\n",
    "Standardize `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data with the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.4) Baseline Model\n",
    "\n",
    "üßëüèª‚Äçüè´ In a regression task, the baseline model **always predicts the average value of `y_train`**\n",
    "\n",
    "<details>\n",
    "    <summary>Really?</summary>\n",
    "    \n",
    "- üêí  Yes, in most cases!\n",
    "- ‚ùóÔ∏è  Be aware that this is not the only possible way of building a baseline model\n",
    "- üíπ  In Time Series, the baseline model predicts the **last seen value**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùìQuestion: what would be the performance of the baseline model here?\n",
    "\n",
    "Before running any Machine Learning algorithm or advanced Deep Learning Neural Networks, it would be great to establish a benchmark score that you are supposed to beat. Otherwise, what is the point of running a fancy algorithm if you cannot beat this benchmark score on the testing set (other than showing off)?\n",
    "\n",
    "Compute the Mean Absolute Error on the testing set using a \"dumb\" prediction of the mean value of `y_train`, computed on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of the baseline model: 6.533042127742185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculate the mean of y_train\n",
    "y_train_mean = np.mean(y_train)\n",
    "\n",
    "# Create an array of the same shape as y_test filled with the mean value\n",
    "y_pred_baseline = np.full(shape=y_test.shape, fill_value=y_train_mean)\n",
    "\n",
    "# Calculate the MAE for the baseline model\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"Mean Absolute Error of the baseline model:\", mae_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìInitializing a Neural Network with a Specific Architecture\n",
    "\n",
    "Write a function called `initialize_model` that generates a Neural Network with 3 layers:\n",
    "- Input layer: **10 neurons**, `relu` activation function, and the appropriate input dimension\n",
    "- Hidden layer: **7 neurons** and the `relu` activation function\n",
    "- Predictive layer: an appropriate layer corresponding to the problem we are trying to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add an input layer with 10 neurons and 'relu' activation function. \n",
    "    # Make sure to specify the input shape that matches your features.\n",
    "    model.add(layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    # Add a hidden layer with 7 neurons and 'relu' activation function\n",
    "    model.add(layers.Dense(7, activation='relu'))\n",
    "\n",
    "    # Add a predictive layer with 1 neuron (for regression). \n",
    "    # No activation function is used for the output layer in regression.\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùìNumber of Parameters\n",
    "\n",
    "How many parameters do we have in this model? \n",
    "1. Compute this number yourself\n",
    "2. Double-check your answer with `model.summary()`\n",
    "\n",
    "We already covered the question about the number of parameters in a fully connected/dense network during **Deep Learning > 01. Fundamentals of Deep Learning** but it is always good to make sure you master the foundations of a new discipline üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                140       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 77        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225\n",
      "Trainable params: 225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 15:45:11.641892: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üí°Answer\n",
    "\n",
    "<details>\n",
    "    <summary>Click here</summary>\n",
    "\n",
    "- Each house has `X_train.shape[-1]` = 13 features\n",
    "- Remember that a neuron is a linear regression combined with an activation function, so we will have 13 weights and 1 bias\n",
    "\n",
    "1. First layer: **10 neurons** $\\times$ (13 weights + 1 bias ) = 140 params\n",
    "2. Second layer: **7 neurons** $\\times$ (10 weights + 1 bias ) = 77 params\n",
    "3. Third layer: **1 neuron** $\\times$ (7 weights + 1 bias) = 8 params\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìCompiling Method\n",
    "\n",
    "Write a function that:\n",
    "1. takes _both_ a **model** and an **optimizer** as arguments\n",
    "2. **compiles** the model\n",
    "3. returns the compiled model\n",
    "\n",
    "Please select wisely:\n",
    "- the **Loss Function** to be optimized\n",
    "- the **metrics** on which the model should be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, optimizer_name):\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam()\n",
    "    # You can add more optimizers here with elif statements\n",
    "\n",
    "    # Compile the model with the selected optimizer, loss function, and metrics\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìEvaluating the Model\n",
    "\n",
    "- Initialize the model and compile it with the `adam` optimizer\n",
    "- Fit it on the training data\n",
    "- Evaluate your model on the testing data\n",
    "\n",
    "Don't forget to use an Early Stopping criterion to avoid overfitting!\n",
    "\n",
    "<details>\n",
    "    <summary>Notes</summary>\n",
    "\n",
    "As we saw in the **\"How to prevent overfitting\"** challenge,  you could also use L2 penalties and Dropout Layers to prevent overfitting but:\n",
    "- Early Stopping is the easiest and quickest code to implement, you just declare `es = EarlyStopping()` and call it back in the `.fit()` step\n",
    "- The main goal of this challenge is to understand the **impact of the optimizer**, so stay focused üòâ\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "11/11 [==============================] - 1s 42ms/step - loss: 574.2426 - mae: 22.1810 - val_loss: 641.5066 - val_mae: 23.6381\n",
      "Epoch 2/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 569.6812 - mae: 22.0587 - val_loss: 636.8500 - val_mae: 23.5224\n",
      "Epoch 3/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 564.9255 - mae: 21.9284 - val_loss: 631.7556 - val_mae: 23.3961\n",
      "Epoch 4/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 559.8851 - mae: 21.7888 - val_loss: 626.1130 - val_mae: 23.2565\n",
      "Epoch 5/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 554.3787 - mae: 21.6366 - val_loss: 620.2362 - val_mae: 23.1083\n",
      "Epoch 6/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 548.4202 - mae: 21.4686 - val_loss: 613.6674 - val_mae: 22.9418\n",
      "Epoch 7/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 541.6144 - mae: 21.2822 - val_loss: 606.1166 - val_mae: 22.7547\n",
      "Epoch 8/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 533.8066 - mae: 21.0768 - val_loss: 597.1538 - val_mae: 22.5415\n",
      "Epoch 9/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 524.6669 - mae: 20.8407 - val_loss: 586.4511 - val_mae: 22.2852\n",
      "Epoch 10/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 513.8161 - mae: 20.5619 - val_loss: 573.1005 - val_mae: 21.9694\n",
      "Epoch 11/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 500.4161 - mae: 20.2296 - val_loss: 556.8322 - val_mae: 21.5883\n",
      "Epoch 12/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 483.8318 - mae: 19.8155 - val_loss: 536.2825 - val_mae: 21.1031\n",
      "Epoch 13/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 462.2578 - mae: 19.3010 - val_loss: 509.5814 - val_mae: 20.4837\n",
      "Epoch 14/150\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 434.2582 - mae: 18.6386 - val_loss: 475.4400 - val_mae: 19.7111\n",
      "Epoch 15/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 399.4794 - mae: 17.8320 - val_loss: 436.7429 - val_mae: 18.8026\n",
      "Epoch 16/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 362.9944 - mae: 16.9185 - val_loss: 396.6921 - val_mae: 17.7947\n",
      "Epoch 17/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 325.8290 - mae: 15.9513 - val_loss: 358.1132 - val_mae: 16.7513\n",
      "Epoch 18/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 288.3369 - mae: 14.9091 - val_loss: 319.0648 - val_mae: 15.6134\n",
      "Epoch 19/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 253.2733 - mae: 13.8445 - val_loss: 282.7731 - val_mae: 14.4456\n",
      "Epoch 20/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 221.1180 - mae: 12.7584 - val_loss: 249.3718 - val_mae: 13.2615\n",
      "Epoch 21/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 191.3437 - mae: 11.6645 - val_loss: 218.8314 - val_mae: 12.0879\n",
      "Epoch 22/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 165.3472 - mae: 10.5695 - val_loss: 193.2296 - val_mae: 10.9659\n",
      "Epoch 23/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 143.1464 - mae: 9.5934 - val_loss: 172.4677 - val_mae: 10.0562\n",
      "Epoch 24/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 124.7277 - mae: 8.7503 - val_loss: 154.5927 - val_mae: 9.2530\n",
      "Epoch 25/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 111.2052 - mae: 8.1528 - val_loss: 141.2868 - val_mae: 8.7094\n",
      "Epoch 26/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 99.8476 - mae: 7.6611 - val_loss: 130.4861 - val_mae: 8.3705\n",
      "Epoch 27/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 91.9363 - mae: 7.3126 - val_loss: 122.5247 - val_mae: 8.1161\n",
      "Epoch 28/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 85.5317 - mae: 7.0313 - val_loss: 116.3641 - val_mae: 7.8995\n",
      "Epoch 29/150\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 80.6474 - mae: 6.7866 - val_loss: 110.7804 - val_mae: 7.6971\n",
      "Epoch 30/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 76.2911 - mae: 6.5787 - val_loss: 105.8639 - val_mae: 7.5138\n",
      "Epoch 31/150\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 72.3389 - mae: 6.3786 - val_loss: 101.6116 - val_mae: 7.3419\n",
      "Epoch 32/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 68.8850 - mae: 6.2123 - val_loss: 97.6126 - val_mae: 7.1906\n",
      "Epoch 33/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 65.6180 - mae: 6.0484 - val_loss: 93.9819 - val_mae: 7.0498\n",
      "Epoch 34/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 62.7399 - mae: 5.8928 - val_loss: 90.2483 - val_mae: 6.9142\n",
      "Epoch 35/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 59.6843 - mae: 5.7350 - val_loss: 86.5132 - val_mae: 6.7923\n",
      "Epoch 36/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.8442 - mae: 5.5780 - val_loss: 82.8721 - val_mae: 6.6587\n",
      "Epoch 37/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 54.1714 - mae: 5.4233 - val_loss: 79.5255 - val_mae: 6.5371\n",
      "Epoch 38/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 51.7948 - mae: 5.2825 - val_loss: 76.6267 - val_mae: 6.4194\n",
      "Epoch 39/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 49.7685 - mae: 5.1453 - val_loss: 73.9132 - val_mae: 6.3074\n",
      "Epoch 40/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 47.8902 - mae: 5.0169 - val_loss: 71.4966 - val_mae: 6.1970\n",
      "Epoch 41/150\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 46.1991 - mae: 4.9054 - val_loss: 69.2262 - val_mae: 6.0879\n",
      "Epoch 42/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 44.6077 - mae: 4.7891 - val_loss: 67.2198 - val_mae: 5.9951\n",
      "Epoch 43/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 43.2480 - mae: 4.6871 - val_loss: 65.0991 - val_mae: 5.9040\n",
      "Epoch 44/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 41.8836 - mae: 4.5923 - val_loss: 62.7447 - val_mae: 5.8073\n",
      "Epoch 45/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 40.5485 - mae: 4.4832 - val_loss: 60.6844 - val_mae: 5.7100\n",
      "Epoch 46/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 39.4611 - mae: 4.3991 - val_loss: 58.7673 - val_mae: 5.6258\n",
      "Epoch 47/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 38.3437 - mae: 4.3202 - val_loss: 57.1498 - val_mae: 5.5502\n",
      "Epoch 48/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 37.4144 - mae: 4.2600 - val_loss: 55.6083 - val_mae: 5.4764\n",
      "Epoch 49/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 36.5246 - mae: 4.1964 - val_loss: 54.1277 - val_mae: 5.4026\n",
      "Epoch 50/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 35.7998 - mae: 4.1421 - val_loss: 52.6232 - val_mae: 5.3277\n",
      "Epoch 51/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 35.0425 - mae: 4.0858 - val_loss: 51.3441 - val_mae: 5.2556\n",
      "Epoch 52/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 34.5040 - mae: 4.0455 - val_loss: 50.1140 - val_mae: 5.1948\n",
      "Epoch 53/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 33.8693 - mae: 3.9984 - val_loss: 49.1453 - val_mae: 5.1651\n",
      "Epoch 54/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 33.3093 - mae: 3.9616 - val_loss: 48.2119 - val_mae: 5.1330\n",
      "Epoch 55/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 32.7523 - mae: 3.9163 - val_loss: 47.0772 - val_mae: 5.0795\n",
      "Epoch 56/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 32.2257 - mae: 3.8798 - val_loss: 46.1894 - val_mae: 5.0405\n",
      "Epoch 57/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 31.8276 - mae: 3.8520 - val_loss: 45.2355 - val_mae: 5.0008\n",
      "Epoch 58/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 31.3541 - mae: 3.8244 - val_loss: 44.4590 - val_mae: 4.9652\n",
      "Epoch 59/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 31.0028 - mae: 3.8043 - val_loss: 43.7037 - val_mae: 4.9393\n",
      "Epoch 60/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 30.5971 - mae: 3.7795 - val_loss: 42.9315 - val_mae: 4.8980\n",
      "Epoch 61/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 4ms/step - loss: 30.1721 - mae: 3.7512 - val_loss: 42.1433 - val_mae: 4.8504\n",
      "Epoch 62/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 29.8637 - mae: 3.7347 - val_loss: 41.3881 - val_mae: 4.8194\n",
      "Epoch 63/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 29.4183 - mae: 3.7107 - val_loss: 40.0901 - val_mae: 4.7763\n",
      "Epoch 64/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 29.0267 - mae: 3.6824 - val_loss: 39.3049 - val_mae: 4.7374\n",
      "Epoch 65/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 28.6985 - mae: 3.6533 - val_loss: 38.7604 - val_mae: 4.7080\n",
      "Epoch 66/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 28.4019 - mae: 3.6300 - val_loss: 38.3121 - val_mae: 4.6871\n",
      "Epoch 67/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 28.1358 - mae: 3.6104 - val_loss: 37.8925 - val_mae: 4.6658\n",
      "Epoch 68/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 27.8710 - mae: 3.5975 - val_loss: 37.6890 - val_mae: 4.6684\n",
      "Epoch 69/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 27.5906 - mae: 3.5807 - val_loss: 37.1736 - val_mae: 4.6350\n",
      "Epoch 70/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 27.2779 - mae: 3.5515 - val_loss: 36.6850 - val_mae: 4.5973\n",
      "Epoch 71/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 27.0046 - mae: 3.5271 - val_loss: 36.0705 - val_mae: 4.5543\n",
      "Epoch 72/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 26.5925 - mae: 3.5091 - val_loss: 35.1922 - val_mae: 4.5232\n",
      "Epoch 73/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 26.2718 - mae: 3.4998 - val_loss: 34.6292 - val_mae: 4.4945\n",
      "Epoch 74/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 26.0051 - mae: 3.4816 - val_loss: 34.1301 - val_mae: 4.4595\n",
      "Epoch 75/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 25.7734 - mae: 3.4653 - val_loss: 33.7469 - val_mae: 4.4402\n",
      "Epoch 76/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 25.5151 - mae: 3.4518 - val_loss: 33.6333 - val_mae: 4.4399\n",
      "Epoch 77/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 25.2636 - mae: 3.4303 - val_loss: 33.2103 - val_mae: 4.4083\n",
      "Epoch 78/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 25.0149 - mae: 3.4071 - val_loss: 32.8537 - val_mae: 4.3838\n",
      "Epoch 79/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 24.7860 - mae: 3.3886 - val_loss: 32.4282 - val_mae: 4.3535\n",
      "Epoch 80/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 24.5262 - mae: 3.3703 - val_loss: 32.0506 - val_mae: 4.3240\n",
      "Epoch 81/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 24.2398 - mae: 3.3484 - val_loss: 31.6332 - val_mae: 4.2918\n",
      "Epoch 82/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 23.9975 - mae: 3.3302 - val_loss: 31.2765 - val_mae: 4.2596\n",
      "Epoch 83/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 23.7912 - mae: 3.3052 - val_loss: 30.8910 - val_mae: 4.2297\n",
      "Epoch 84/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 23.5604 - mae: 3.2847 - val_loss: 30.5963 - val_mae: 4.2141\n",
      "Epoch 85/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 23.3380 - mae: 3.2689 - val_loss: 30.3441 - val_mae: 4.2016\n",
      "Epoch 86/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 23.0942 - mae: 3.2550 - val_loss: 30.1061 - val_mae: 4.2002\n",
      "Epoch 87/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.8885 - mae: 3.2454 - val_loss: 29.7112 - val_mae: 4.1993\n",
      "Epoch 88/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 22.6800 - mae: 3.2399 - val_loss: 29.3863 - val_mae: 4.1949\n",
      "Epoch 89/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.4162 - mae: 3.2256 - val_loss: 29.0170 - val_mae: 4.1673\n",
      "Epoch 90/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.2011 - mae: 3.2061 - val_loss: 28.7033 - val_mae: 4.1379\n",
      "Epoch 91/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 21.9511 - mae: 3.1765 - val_loss: 28.3070 - val_mae: 4.0952\n",
      "Epoch 92/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 21.7170 - mae: 3.1609 - val_loss: 27.8528 - val_mae: 4.0665\n",
      "Epoch 93/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 21.5073 - mae: 3.1449 - val_loss: 27.4060 - val_mae: 4.0389\n",
      "Epoch 94/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 21.3712 - mae: 3.1510 - val_loss: 26.7281 - val_mae: 4.0192\n",
      "Epoch 95/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 21.0314 - mae: 3.1323 - val_loss: 26.3926 - val_mae: 3.9905\n",
      "Epoch 96/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.8219 - mae: 3.1096 - val_loss: 26.1437 - val_mae: 3.9797\n",
      "Epoch 97/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.6864 - mae: 3.1029 - val_loss: 25.9432 - val_mae: 3.9665\n",
      "Epoch 98/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 20.5071 - mae: 3.0879 - val_loss: 25.6532 - val_mae: 3.9455\n",
      "Epoch 99/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 20.3391 - mae: 3.0708 - val_loss: 25.3223 - val_mae: 3.9223\n",
      "Epoch 100/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.1454 - mae: 3.0557 - val_loss: 24.9945 - val_mae: 3.8926\n",
      "Epoch 101/150\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.0054 - mae: 3.0443 - val_loss: 24.6767 - val_mae: 3.8766\n",
      "Epoch 102/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 19.7935 - mae: 3.0397 - val_loss: 24.2192 - val_mae: 3.8683\n",
      "Epoch 103/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.6572 - mae: 3.0266 - val_loss: 23.8954 - val_mae: 3.8379\n",
      "Epoch 104/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.4430 - mae: 3.0036 - val_loss: 23.5428 - val_mae: 3.8108\n",
      "Epoch 105/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 19.2974 - mae: 2.9918 - val_loss: 23.2775 - val_mae: 3.7861\n",
      "Epoch 106/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 19.1525 - mae: 2.9786 - val_loss: 23.0678 - val_mae: 3.7745\n",
      "Epoch 107/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.0124 - mae: 2.9651 - val_loss: 22.8472 - val_mae: 3.7547\n",
      "Epoch 108/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 18.8560 - mae: 2.9504 - val_loss: 22.7350 - val_mae: 3.7502\n",
      "Epoch 109/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 18.7439 - mae: 2.9378 - val_loss: 22.5312 - val_mae: 3.7218\n",
      "Epoch 110/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 18.5811 - mae: 2.9220 - val_loss: 22.2947 - val_mae: 3.7047\n",
      "Epoch 111/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 18.4692 - mae: 2.9110 - val_loss: 22.0443 - val_mae: 3.6767\n",
      "Epoch 112/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 18.3324 - mae: 2.8964 - val_loss: 21.8661 - val_mae: 3.6647\n",
      "Epoch 113/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 18.2173 - mae: 2.8912 - val_loss: 21.7688 - val_mae: 3.6628\n",
      "Epoch 114/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 18.0872 - mae: 2.8804 - val_loss: 21.6245 - val_mae: 3.6509\n",
      "Epoch 115/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17.9524 - mae: 2.8620 - val_loss: 21.5417 - val_mae: 3.6387\n",
      "Epoch 116/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17.8446 - mae: 2.8464 - val_loss: 21.3944 - val_mae: 3.6254\n",
      "Epoch 117/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 17.7355 - mae: 2.8342 - val_loss: 21.2329 - val_mae: 3.6151\n",
      "Epoch 118/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17.6008 - mae: 2.8262 - val_loss: 21.0048 - val_mae: 3.6014\n",
      "Epoch 119/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17.4606 - mae: 2.8177 - val_loss: 20.7931 - val_mae: 3.5839\n",
      "Epoch 120/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 17.3601 - mae: 2.8021 - val_loss: 20.6815 - val_mae: 3.5745\n",
      "Epoch 121/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17.2113 - mae: 2.7906 - val_loss: 20.5462 - val_mae: 3.5684\n",
      "Epoch 122/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 5ms/step - loss: 17.0780 - mae: 2.7817 - val_loss: 20.3823 - val_mae: 3.5518\n",
      "Epoch 123/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 16.9583 - mae: 2.7770 - val_loss: 20.0650 - val_mae: 3.5192\n",
      "Epoch 124/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 16.8443 - mae: 2.7730 - val_loss: 19.8679 - val_mae: 3.5016\n",
      "Epoch 125/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 16.7100 - mae: 2.7641 - val_loss: 19.6969 - val_mae: 3.4880\n",
      "Epoch 126/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 16.5882 - mae: 2.7505 - val_loss: 19.4786 - val_mae: 3.4702\n",
      "Epoch 127/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 16.4752 - mae: 2.7342 - val_loss: 19.3235 - val_mae: 3.4581\n",
      "Epoch 128/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 16.3626 - mae: 2.7191 - val_loss: 19.1627 - val_mae: 3.4499\n",
      "Epoch 129/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 16.2617 - mae: 2.7137 - val_loss: 18.9703 - val_mae: 3.4347\n",
      "Epoch 130/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 16.1354 - mae: 2.6977 - val_loss: 18.6498 - val_mae: 3.3993\n",
      "Epoch 131/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 16.0690 - mae: 2.6869 - val_loss: 18.4478 - val_mae: 3.3795\n",
      "Epoch 132/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.9403 - mae: 2.6925 - val_loss: 18.1598 - val_mae: 3.3621\n",
      "Epoch 133/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 15.8104 - mae: 2.7054 - val_loss: 18.0730 - val_mae: 3.3584\n",
      "Epoch 134/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.7210 - mae: 2.6971 - val_loss: 17.9279 - val_mae: 3.3436\n",
      "Epoch 135/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.6745 - mae: 2.6996 - val_loss: 17.5059 - val_mae: 3.2831\n",
      "Epoch 136/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.5274 - mae: 2.6813 - val_loss: 17.4992 - val_mae: 3.2845\n",
      "Epoch 137/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.3842 - mae: 2.6634 - val_loss: 17.4954 - val_mae: 3.2957\n",
      "Epoch 138/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 15.2654 - mae: 2.6475 - val_loss: 17.5848 - val_mae: 3.3181\n",
      "Epoch 139/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.2003 - mae: 2.6462 - val_loss: 17.4189 - val_mae: 3.2998\n",
      "Epoch 140/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 15.1703 - mae: 2.6320 - val_loss: 17.3973 - val_mae: 3.2941\n",
      "Epoch 141/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 15.0006 - mae: 2.6080 - val_loss: 17.2315 - val_mae: 3.2765\n",
      "Epoch 142/150\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 14.9358 - mae: 2.6031 - val_loss: 17.0354 - val_mae: 3.2544\n",
      "Epoch 143/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.8091 - mae: 2.5907 - val_loss: 16.9638 - val_mae: 3.2519\n",
      "Epoch 144/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.7530 - mae: 2.5962 - val_loss: 17.0353 - val_mae: 3.2666\n",
      "Epoch 145/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.6660 - mae: 2.5916 - val_loss: 16.8285 - val_mae: 3.2438\n",
      "Epoch 146/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.5227 - mae: 2.5750 - val_loss: 16.6355 - val_mae: 3.2181\n",
      "Epoch 147/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.4746 - mae: 2.5671 - val_loss: 16.8669 - val_mae: 3.2532\n",
      "Epoch 148/150\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 14.4063 - mae: 2.5653 - val_loss: 16.7729 - val_mae: 3.2423\n",
      "Epoch 149/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.3185 - mae: 2.5521 - val_loss: 16.5740 - val_mae: 3.2222\n",
      "Epoch 150/150\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 14.2443 - mae: 2.5392 - val_loss: 16.3172 - val_mae: 3.1887\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 21.5397 - mae: 3.5829\n",
      "Test Loss: 21.53965187072754, Test MAE: 3.582941770553589\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = initialize_model()\n",
    "\n",
    "# Compile the model with the Adam optimizer\n",
    "model = compile_model(model, 'adam')\n",
    "\n",
    "# Declare an EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Fit the model on the training data\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    validation_split=0.2, # using part of the training data for validation\n",
    "                    epochs=150, \n",
    "                    callbacks=[early_stopping],\n",
    "                    batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network vs. Baseline\n",
    "\n",
    "Compare the MAE on the testing set between this Neural Network and the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_absolute_error_test_baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mae_test_baseline \u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error_test_baseline\u001b[49m\n\u001b[1;32m      2\u001b[0m mae_test_neuralnet \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe MAE on the test is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae_test_neuralnet\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for the Neural Network vs. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae_test_baseline\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for the baseline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_absolute_error_test_baseline' is not defined"
     ]
    }
   ],
   "source": [
    "mae_test_baseline = mean_absolute_error_test_baseline\n",
    "mae_test_neuralnet = res\n",
    "\n",
    "print(f\"The MAE on the test is {mae_test_neuralnet:.4f} for the Neural Network vs. {mae_test_baseline:.4f} for the baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Which Optimizer is the Best? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùìTrying Different Optimizers\n",
    "\n",
    "Re-run the same model on the same data using different optimizers (in a `for` loop). \n",
    "\n",
    "For each optimizer:\n",
    "- üìâ Plot the history of the loss (MSE) and the metric (MAE)\n",
    "    - üéÅ We coded two functions: `plot_loss_mae` and `plot_loss_mse`. Which one should you use? Feel free to use it.\n",
    "- ‚úçÔ∏è Report the corresponding Mean Absolute Error\n",
    "- ‚è≥ Compute the time your Neural Net needed to fit the training set\n",
    "\n",
    "üìö [`tensorflow.keras.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mae(history):\n",
    "    # Setting figures\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "\n",
    "    # Create the plots\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "\n",
    "    ax2.plot(history.history['mae'])\n",
    "    ax2.plot(history.history['val_mae'])\n",
    "\n",
    "    # Set titles and labels\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "\n",
    "    ax2.set_title('MAE')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    # Set limits for y-axes\n",
    "    ax1.set_ylim(ymin=0, ymax=200)\n",
    "    ax2.set_ylim(ymin=0, ymax=20)\n",
    "\n",
    "    # Generate legends\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "\n",
    "    # Show grids\n",
    "    ax1.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax1.grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    ax2.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax2.grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mse(history):\n",
    "    # Setting figures\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "\n",
    "    # Create the plots\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "\n",
    "    ax2.plot(history.history['mse'])\n",
    "    ax2.plot(history.history['val_mse'])\n",
    "\n",
    "    # Set titles and labels\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "\n",
    "    ax2.set_title('MSE')\n",
    "    ax2.set_ylabel('MSE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    # Set limits for y-axes\n",
    "    ax1.set_ylim(ymin=0, ymax=20)\n",
    "    ax2.set_ylim(ymin=0, ymax=200)\n",
    "\n",
    "    # Generate legends\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "\n",
    "    # Show grids\n",
    "    ax1.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax1.grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    ax2.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax2.grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network vs. Baseline - Part 2\n",
    "\n",
    "Are your predictions better than those of the baseline model you evaluated at the beginning of the notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# MAE on the testing set for different optimizers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m optimizer, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madagrad\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mresults\u001b[49m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe MAE on the test set with the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m optimizer is equal to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# MAE on the testing set for the baseline model    \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# MAE on the testing set for different optimizers\n",
    "for optimizer, result in zip(['rmsprop', 'adam', 'adagrad'], results):\n",
    "    print(f\"The MAE on the test set with the {optimizer} optimizer is equal to {result:.2f}\")\n",
    "\n",
    "# MAE on the testing set for the baseline model    \n",
    "print(\"-\"*5)    \n",
    "print(f\"The MAE on the test set with the baseline model is equal to {mae_test_baseline:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí°Answer\n",
    "\n",
    "<details>\n",
    "    <summary>Click me</summary>\n",
    "\n",
    "You can see that the Neural Network beat the baseline when using either `adam` or `rmsprop` as an optimizer but the result was significantly worse with the `adagrad` optimizer.\n",
    "\n",
    "**üëá The advice from the Deep Learning community is the following üëá:**\n",
    "\n",
    "üî• So far, our best-performing optimizer is `adam`. Maybe a mathematician specialized in numerical methods will find a better solver in the future but for the moment, Adam is your best friend and they have already been helping us achieve remarkable results.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps for this Challenge\n",
    "\n",
    "üë©‚Äçüéì Do you remember the **Machine Learning > 04.Under The Hood** unit where we coded our **Gradient Descent** by choosing a specific `learning_rate`? It represents how slow/fast your algorithm learns. In other words, it controls the intensity of the change of the weights at each optimization of the NN, at each backpropagation!\n",
    "\n",
    "üöÄ Well, the **solvers** in Machine Learning and the **optimizers** in Deep Learning are advanced iterative methods relying on **hyperparameters**, and the `learning_rate` is one of them!\n",
    "\n",
    "ü§î How can I control this `learning_rate`?\n",
    "\n",
    "‚úÖ Instead of calling an optimizer with a string (\"adam\", \"rmsprop\", etc.), which uses a default value for the Learning Rate, we will call üìö [`tf.keras.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) objects üìö and tailor them to our needs.\n",
    "\n",
    "üßëüèª‚Äçüè´ Different Learning Rates have different consequences, as shown here: \n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3-eu-west-1.amazonaws.com/06-DL/02-Optimizer-loss-and-fitting/learning_rate.png\" alt=\"Learning rate\" height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) The Influence of the Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting an Optimizer with a Custom Learning Rate\n",
    "\n",
    "üìö [`tf.keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n",
    "\n",
    "Instead of initializing the optimizer with a string, let's initialize an optimizer manually.\n",
    "\n",
    "- Instantiate an Adam optimizer with a Learning Rate of $ \\alpha = 0.1$\n",
    "    - Keep the other parameters at their default values\n",
    "- Use this optimizer in the `compile_model` function\n",
    "- Train/fit the model\n",
    "- Plot the history\n",
    "- Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'optimizer' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:8\u001b[0m\n",
      "Cell \u001b[0;32mIn [17], line 7\u001b[0m, in \u001b[0;36mcompile_model\u001b[0;34m(model, optimizer_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m Adam()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# You can add more optimizers here with elif statements\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Compile the model with the selected optimizer, loss function, and metrics\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[43moptimizer\u001b[49m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'optimizer' referenced before assignment"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Instantiating the Adam optimizer with a learning rate alpha = 0.1\n",
    "adam = Adam(learning_rate = 0.1)\n",
    "\n",
    "# 2. Initializing the model\n",
    "model = initialize_model()\n",
    "\n",
    "# 3. Compiling the model with the custom Adam optimizer\n",
    "model = compile_model(model, adam)\n",
    "\n",
    "# 4. Training the neural net\n",
    "es = EarlyStopping(patience = 10)\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train, \n",
    "    validation_split = 0.3,\n",
    "    shuffle = True,\n",
    "    batch_size=16, \n",
    "    epochs = 1_000,\n",
    "    callbacks = [es],\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "# 5. Plot the history and evaluate the model\n",
    "res = model.evaluate(X_test_scaled, y_test)[1]\n",
    "res\n",
    "print(f'Mean absolute error with a learning rate of 0.1: {res:.4f}')\n",
    "plot_loss_mae(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìPlaying with Learning Rates\n",
    "\n",
    "Now, reproduce the same plots and results but for different Learning Rates.\n",
    "\n",
    "<details>\n",
    "    <summary>Remark</summary>\n",
    "\n",
    "There is a chance that the y-axis is too large for you to visualize some results with some Learning Rates. In that case, feel free to re-write the plot function to plot only the epochs $> 10$.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 2]\n",
    "results = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) The Loss\n",
    "\n",
    "---\n",
    "\n",
    "‚ùóÔ∏èIt's important to clearly understand the **difference between losses and metrics**‚ùóÔ∏è\n",
    "\n",
    "- üèãüèª‚Äç‚ôÄÔ∏è The **Loss Functions** are computed **during the training procedure**\n",
    "    - For regression tasks, the classic Loss Functions are **(Root) Mean Squared Error** ((R)MSE), **Mean Absolute Error** (MAE), and **Mean Squared Logarithmic Error** (MSLE, as seen in the Kaggle challenge)\n",
    "    - For classification tasks, the classic Loss Functions are **Binary Crossentropy** (also known as LogLoss), **Categorical Crossentropy**, Hinge Loss, etc.\n",
    "- üßëüèª‚Äçüè´ The **metrics** are computed to evaluate your models, **after training them**!\n",
    "    - For regression tasks, common metrics are MSE, MAE, RMSE, Coefficient of Determination (R2), etc.\n",
    "    - For classification tasks, common metrics are Accuracy, Recall, Precision, and F1-Score\n",
    "- üëÄ Notice that some metrics can also be used as Loss Functions, as long as they are differentiable! (e.g. the **MSE**)\n",
    "\n",
    "If these notions are not clear, we strongly advise reviewing **Machine Learning > 03.Performance Metrics** and **Machine Learning > 05.Model Tuning**\n",
    "\n",
    "---\n",
    "\n",
    "‚è© Alright, after this reminder, let's move on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìOptimizing a Model with a certain Loss Function\n",
    "\n",
    "- Run the same NN, once with `mae` as the loss, and once with `mse`\n",
    "- In both cases, compare `mae_train`, `mae_val`, `mse_train`, `mse_val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí°Learnings\n",
    "\n",
    "ü§î When you work on this regression task, you want to achieve the lowest MAE in the testing set at the end, right? So why wouldn't we use it directly as a Loss Function that would decrease with the number of epochs? \n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "Well, even the Deep Learning research community is still trying to answer these types of questions rigorously.\n",
    "\n",
    "One thing is sure: in Deep Learning, you will never really reach the \"global minimum\" of the true Loss Function (the one computed using your entire training set as one single batch). So, in your first model (minimizing the MAE loss), your global MAE minimum has clearly **not** been reached (otherwise you could never beat it). \n",
    "\n",
    "Why? It may well be that the minimization process of the second model has performed better. Maybe because the Loss Function \"energy map\" is \"smoother\" or more \"convex\" in the case of MSE loss? Or maybe your hyper-parameters are best suited to the MSE than to the MAE loss?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)  Saving and Loading a Trained Neural Network\n",
    "\n",
    "ü§Ø Imagine that you trained a complex Neural Network (many layers/neurons) on a huge dataset. The parameters of your Deep Learning Model (weights and biases) are now optimized and you would like to share these weights with a teammate who wants to predict a new data point. Would you give this person your notebook for them to run it and then predict the new data point? Hell no, we have a much better solution:\n",
    "- üíæ Save the weights of the optimized Neural Network\n",
    "- ü§ù Your friend/colleague/teammate/classmate can use them to predict a new data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìTraining a Good Model\n",
    "\n",
    "- Try to reach an MAE on the testing set that is lower than 5 (_feel free to re-create the architecture and redefine your compiling parameters in this section!_)\n",
    "    - Remember: we are predicting house prices, so a mistake of less than 5.000 USD is already good in the real estate industry)\n",
    "\n",
    "- Whether you managed to reach it or not, move on to the \"Saving a Model\" section after a few attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'solution',\n",
    "    mae_test = mae_test\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìSaving a Model\n",
    "\n",
    "üìö [`tf.keras.models.save_model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model)\n",
    "\n",
    "Save your model using the `.save_model(model, 'name_of_my_model')` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìLoading a Model\n",
    "\n",
    "üìö [`tf.keras.models.load_model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model)\n",
    "\n",
    "- Load the model that you've just saved using `.load_model('name_of_your_model')` and store it into a variable called `loaded_model`\n",
    "- Evaluate it on the testing data to check that it gives the same result as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) (Optional) Exponential Decay\n",
    "\n",
    "‚ùóÔ∏èWarning‚ùóÔ∏è \n",
    "\n",
    "- This section is optional and for advanced practitioners\n",
    "- The next question is not essential and can be indeed skipped as many algorithms can be run without such optimization\n",
    "\n",
    "üßëüèª‚Äçüè´ Instead of keeping a fixed Learning Rate, you can change it from one iteration to the other, with the intuition that at first, you need a large Learning Rate to learn fast, and as the Neural Network converges and gets closer to the minimum of the Loss Function, you can decrease the value of the Learning Rate. This is called a **scheduler**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìThe Exponential Decay Scheduler\n",
    "\n",
    "Use the üìö [Exponential Decay Scheduler](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) üìö in the `adam` optimizer, and run it on the previous data.\n",
    "\n",
    "Start with the following:\n",
    "\n",
    "```python\n",
    "initial_learning_rate = 0.001 # start with default Adam value\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    # Every 5000 iterations, multiply the learning rate by 0.7\n",
    "    initial_learning_rate, decay_steps = 5000, decay_rate = 0.7,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0] * 0.7 / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Congratulations!\n",
    "\n",
    "üíæ Do not forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move to the next challenge!\n",
    "\n",
    "---\n",
    "\n",
    "**Further reading after your bootcamp:**\n",
    "\n",
    "The **Boston Housing Dataset** was deprecated by Scikit-Learn for ethical reasons, and TensorFlow may also replace it in the future.\n",
    "\n",
    "A certain M. Carlisle wrote a 12-min read article called [\"Racist Data Destruction\"](https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8) to investigate this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
